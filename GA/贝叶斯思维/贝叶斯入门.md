# 贝叶斯思维入门

## 1. 贝叶斯定理
### 1.1 条件概率

&emsp;&emsp;条件概率是指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P（A|B），读作“在B的条件下A的概率”。若只有两个事件A，B，那么，
        ![Image](https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D113/sign=2835f4587c1ed21b7dc92ae49e6fddae/b64543a98226cffcb1b6da18b8014a90f703eaf8.jpg)  。

### 1.2 联合概率

&emsp;&emsp;联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。假设X和Y都服从正态分布，那么P{X<4,Y<0}就是一个联合概率，表示X<4,Y<0两个条件同时成立的概率。表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P（A∩B）。

        p(A and B) = p(A) * p(A|B) 

### 1.3  贝叶斯定理

&emsp;&emsp;贝叶斯定理也称贝叶斯推理，早在18世纪，英国学者贝叶斯(1702～1763)曾提出计算条件概率的公式用来解决如下一类问题：假设H[1],H[2]…,H[n]互斥且构成一个完全事件，已知它们的概率P(H[i]),i=1,2,…,n,现观察到某事件A与H[1],H[2]…,H[n]相伴随机出现，且已知条件概率P(A/H[i])，求P(H[i]/A)。

&emsp;&emsp;公式：
![Image](https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D215/sign=07ef5cb4b03eb13540c7b0ba931fa8cb/1ad5ad6eddc451da062e7cb7bafd5266d1163295.jpg)  

### 1.4 定理解析
&emsp;&emsp;公式简写：
![Image](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1537094867417&di=23338659ab9ca1006fda0105444d6bfd&imgtype=0&src=http%3A%2F%2Fimg2.ph.126.net%2FmSFqeu-RjVioAbdUJ7MvVA%3D%3D%2F1916000166569836277.jpg)

    P(A):先验概率，即得到新数据前某一假设的概率。
    p(A|B):后验概率，即看到新数据（条件）后，我们要计算的的该假设的概率。
    p(B|A):似然度，即在该假设作为前提时，得到新数据（条件）的概率。
    p(B):标准化常量：即在任何假设下得到这一数据（条件）的概率

### 1.5 曲奇饼问题
    Q：
        假设有两碗曲奇饼，碗1包含30个香草曲奇饼和10个巧克力曲奇饼，碗2有香草曲奇饼10个和巧克力曲奇饼10个。碗1和碗2都放在黑箱中，问：从黑箱里任选一个碗抓一个饼抓到了香草味，求来自碗1的概率? 

    分析：
        假设：取到香草曲奇饼来自碗1（p(B1|A1)）
        数据化：A：曲奇饼 B：碗
               A1：香草味 A2：巧克力
               B1：碗1    B2：碗2
        带值：先验概率 P（B1）：1/2
             似然度 p(A1|B1) ： 3/4
             标准化常量 p(A1) : 5/8


### 1.6 M&M豆豆问题
    Q：
        M&M豆是有各种颜色的糖果巧克力豆。制造M&M豆的Mars公司会不时变更不同颜色巧克力豆之间的混合比例。
        1995年，他们推出了蓝色的M&M豆。在此前一袋普通的M＆M豆中，颜色的搭配为：30%褐色，20%黄色，20%红色，10%绿色，10%橙色，10%黄褐色。这之后变成了：24%蓝色，20%绿色，16%橙色，14%黄色，13%红色，13%褐色。
        假设我的一个朋友有两袋M&M豆，他告诉我一袋是1994年，一袋是1995年。
        但他没告诉我具体哪个袋子是哪一年的，他从每个袋子里各取了一个M&M豆给我。一个是黄色，一个是绿色的。那么黄色豆来自1994年的袋子的概率是多少？

    分析：
        假设：取到黄豆来自1994年的袋子（p(B1|A1)）
        数据化：A：豆豆 B：袋子
               A1：黄豆  A2：绿豆  A3...
               B1：1994   B2：1996
        带值：先验概率 P（B1）：1/2
             似然度 p(A1|B1) ： 0.2
             标准化常量 p(A1) : 0.17

        It's a trap!

        假设：取到黄豆来自1994年的袋子（p(B1|A1 and B2|A2)）,同时绿豆来自1996
        数据化：A：豆豆 B：袋子
               A1：黄豆  A2：绿豆  A3...
               B1：1994   B2：1996
        带值：先验概率 P（B1）：1/2
             似然度 p(A1|B1)*p(A2|B2) ： 0.2*0.2
             标准化常量 p(A1)+p(A2) : 0.02+0.1*0.14*0.5 = 0.027 

### 1.7 蒙蒂大厅
    Q： 
        蒙蒂大厅（Monty Hall problem）难题可能是历史上最有争议的概率问题。问题看似简单，但正确答案如此有悖常理以致很多人不能接受，很多聪明人都难堪于自己搞错了反而据理力争，而且是公开的。
        蒙蒂大厅是游戏节目“来做个交易”（Let’s Make a Deal）的主场。蒙蒂大厅难题也是这一节目的常规游戏之一。如果你参加节目，规则是这样的：
        蒙蒂向你示意三个关闭的大门，然后告诉你每个门后都有一个奖品：一个奖品是一辆车，另外两个是像花生酱和假指甲这样不值钱的奖品。奖品随机配置。
        游戏的目的是要猜哪个门后有车。如果你猜对了就可以拿走汽车。
        你先挑选一扇门，我们姑且称之为门A，其他两个称为门B和门C。
        在打开你选中的门前，为了增加悬念，蒙蒂会先打开B或C中一个没有车的门来增加悬念（如果汽车实际上就是在A门背后，那么蒙蒂打开门B或门C都是安全的，所以他可以随意选择一个）。
        然后蒙蒂给你一个选择。坚持最初的选择还是换到剩下的未打开的门上。
        问题是，你应该“坚持”还是“换”？有没有区别？

        假设：选择C门（p(A3|B) ）
        数据化：A：门 B：排除
               A1：A  A2：B  A3：C
               B：B
        带值：先验概率 P（A3）：1/3
             似然度 p(B|A3) ：1
                   p(B|A2) : 0
                   p(B|A1) : 1/2
             标准化常量 p(B) : 1/3+0+1/6 = 1/2

# 2.贝叶斯框架

## 2.1 贝叶斯框架

    class BayesFrame(Pmf):

        # def __init__ (self, hypos, alpha = 1.0):
        #     Pmf.__init__(self)
        #     for hypo in hypos:
        #         self.Set(hypo, hypo**(-alpha))
        #     self.Normalize()

        def __init__(self,hypos):
            Pmf.__init__(self)
            for hypo in hypos:
                self.Set(hypo,1)

            self.Normalize()

        def Update (self,data):
            for hypo in self.Values():
                like = self.Likelihood(data,hypo)
                self.Mult(hypo,like)

            self.Normalize()

        def Print (self):
            for hypo , prob in self.Items():
                print(hypo, prob)

## 2.2 曲奇饼问题

    class Cooike(BayesFrame):
    mixes = {
        'Bowl 1' : dict(vanilla = 0.75, chocolate = 0.25),
        'Bowl 2' : dict(vanilla = 0.5, chocolate = 0.5)
    }

    def Likelihood(self,data,hypo):
        mix = self.mixes[hypo]
        like = mix[data]

        return like

    hypos = ['Bowl 1', 'Bowl 2']
    cooike = Cooike(hypos)
    cooike.Update('vanilla')
    cooike.Print()

# 3. 估计（模糊的先验概率与区间估算）

## 3.1 火车头问题
    Q：
        铁路上以1到N命名火车头。有一天你看到一个标号60的火车头，请估计铁路上有多少火车头？

    分析：
        要应用贝叶斯进行推理，我们可以将这个问题分成两步骤进行：
        1．在得到数据之前，我们对N的认识是什么？
        2．已知一个N的任意值后，得到数据（“标志为60号的火车头”）的似然度？
        第一个问题的答案就是问题的前置概率。第二个问题是似然度。
        在选择前置概率上，我们还没有太多的基本信息，但我们可以从一些简单情况开始，再考虑进一步的方法。假设N可以是从1到1000等概率的任何值。

    A:
        class Train(BayesFrame):

        """Represents hypotheses about how many trains the company has.

        The likelihood function for the train problem is the same as
        for the Dice problem.
        """
        def Likelihood(self,data,hypo):
            if hypo<data:
                return 0
            else:
                return 1.0/hypo

        hypos = range(1, 1001)
        suite = Train(hypos)

        suite.Update(60)

     图：


        不过，这还不是我们的目标。另一个可选的方法是计算后验概率的平均值分布，后验的平均值是333，所以要是你想最大限度地减少错误，这也许是一个很好的猜测结果。


## 3.2 先验概率的改变

        上限             后验均值

         500               207
        1000               333
        2000               552

    列车60之后加入列车30与90：

        上限              后验均值

         500               152
        1000               164
        2000               171


    幂律
          所谓幂律，是说节点具有的连线数和这样的节点数目乘积是一个定值，也就是几何平均是定值，比如有10000个连线的大节点有10个，有1000个连线的中节点有100个，100个连线的小节点有1000个……，在对数坐标上画出来会得到一条斜向下的直线。

          公式：PMF（X）》（1/x）的-a次方

    取 a = 1 后改变：

            def __init__ (self, hypos, alpha = 1.0):
            Pmf.__init__(self)
            for hypo in hypos:
                self.Set(hypo, hypo**(-alpha))
            self.Normalize()

    
        上限              后验均值（>>134）

         500               131
        1000               133
        2000               134

### 3.3 置信区间与累积分布函数
    
    置信区间：
        置信区间是指由样本统计量所构造的总体参数的估计区间。在统计学中，一个概率样本的置信区间（Confidence interval）是对这个样本的某个总体参数的区间估计。置信区间展现的是这个参数的真实值有一定概率落在测量结果的周围的程度，其给出的是被测量参数的测量值的可信程度，即前面所要求的“一个概率”。



    取5%与95%，90%的可能落入区间（91.243）

    cdf = suite.MakeCdf()
    interval = cdf.Percentile(5), cdf.Percentile(95)


## 贡献人员名单

名单按照字母顺序排序。

* [Gasen](https://github.com/GasenLi)

## CHANGELOG

* v1.0 2018/09/16 无图版